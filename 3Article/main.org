# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE:
#+LANGUAGE:  en
#+STARTUP: overview indent inlineimages logdrawer
#+OPTIONS: H:5 author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

# #+LATEX_CLASS: IEEEtran
#+LaTeX_CLASS: article
# #+LaTeX_CLASS: acm-proc-article-sp

#+BABEL: :session *R* :cache yes :results output graphics :exports both :tangle yes 

* Introduction
* Materials and methods
** Model
Following the common notations in linear latent factor regression models cite
here , for an observation $i \in \{1, ..., n\}$, we assume that the random
vector $G_i = (G_{i 1},..., G_{i L})$ assuming the co-variable $X_i = (X_{i 1},
..., X_q{i d})$ is a multivariate normal distribution such that
   
$$ E[G_i | X_i] = X_i B^T $$
   
where $B$ is the unknown regression coefficient. Then assuming $K$ lattent
factors we write the covariance matrix $var(G_i|X_i) = \Sigma$ as follow
 
$$ \Sigma = D + V V^T $$ 
   
where $V$ is $L \times K$ matrix of latent
factor loadings and $D$ is the diagonal matrix of size $L$. We can write the
following matrix notation of the model: 
   
$$ G = U V^T + X B^T + E $$ 
   
where $U$ is a $n \times K$ matrix of latent factor scores and $E$ is the error
matrix distributed with a multivariate normal distribution with the diagonal
covariance matrix $D$.
   
** Estimation
To estimate model parameter we propose the following optimization problem
$$ min_{rk(C) \leq K} \frac{1}{2} ||G - C - X B^T||_{F}^2 + \lambda r(B) $$
where $C$ is the latent matrix such that $C = U V^T$, $||.||_F$ the frobenius
matrix ,$rk(.)$ the function which returns the rank of a matrix and $r$ a
convex regularization function. In this article we discuss the the ridge
regularization function $||.||^2_2$ and the lasso regularization function
$||.||$.
 
** Algorithm
   
In this section we propose algorithms to estimate latent matrices $U$ and $V$
and association parameter $B$. We present a very efficient algorithm for the
ridge regularized loss function based on analytic solutions of the
optimization problem. We also present an efficient alternated algorithm for
data with missing values and a general regularization convex function.

*** Analytic solution

*** Alternated algorithm
    
*** Missing values
It is frequent that there are missing values in the data matrix. A solution
is to use an imputation algoritm before running the association study. But
this can lead to bias estimation and spurious association pattern introduced
by the imputation algorithm. We propose an algorithm we avoid using a
preliminary imputation method.

** Hypothesis testing

** Simulations and dataset

*** Generative model simulation
We used equation to generate generative model dataset. The latent factor
scores and loadings $U$ and $V$ were generated using a multivariate gaussian
distribution with a zero mean and a $K$ identity matrix for the covariance
matrix where is the number of latent factor. The error matrix $E$ was
generated using a multivariate gaussian distribution with a zero mean and a
$L$ identity matrix for the covariance matrix where $L$ is the number of
variables. The co-variable $X$ was generated with a normal distribution with
the mean egual to zero and the standard deviation egual to one such that the
Pearson linear correlation between $X$ and $U_1$ the first latent score
matrix equal to $c$.

* Results
* Discussion
* Figures and tables
** Comparison of method
#+begin_src R :results output :session *RArticle* :exports none
require(ThesisRpackage)
exp <- retrieveExperiment(100)
#+end_src

#+begin_src R :results output graphics :file Rplots/Rplots.png :exports none :width 600 :height 400 :session *RArticle* 
Article3_MethodComparison_plot_AUC(exp)
#+end_src

#+RESULTS:
[[file:Rplots/Rplots.png]]



