# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE:
#+LANGUAGE:  en
#+STARTUP: overview indent inlineimages logdrawer
#+OPTIONS: H:5 author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

# #+LATEX_CLASS: IEEEtran
#+LaTeX_CLASS: article
# #+LaTeX_CLASS: acm-proc-article-sp

#+HTML_MATHJAX: align: left indent: 5em tagside: left font: Neo-Euler

#+BABEL: :session *R* :cache yes :results output graphics :exports both :tangle yes 


* Introduction

* Materials and methods
** COMMENT Model
Following the common notations in linear latent factor regression models cite
here , for an observation $i \in \{1, ..., n\}$, we assume that the random
vector $G_i = (G_{i 1},..., G_{i L})$ assuming the co-variable $X_i = (X_{i 1},
..., X_q{i d})$ is a multivariate normal distribution such that
   
$$ E[G_i | X_i] = X_i B^T $$
   
where $B$ is the unknown regression coefficient. Then assuming $K$ latent
factors we write the covariance matrix $var(G_i|X_i) = \Sigma$ as follow
 
$$ \Sigma = D + V V^T $$ 
   
where $V$ is $L \times K$ matrix of latent
factor loadings and $D$ is the diagonal matrix of size $L$. We can write the
following matrix notation of the model: 
   
$$ G = U V^T + X B^T + E $$ 
   
where $U$ is a $n \times K$ matrix of latent factor scores and $E$ is the error
matrix distributed with a multivariate normal distribution with the diagonal
covariance matrix $D$.
** Model 
Following the common notation in linear factor regression we write the following
model 

\begin{equation}
\label{eq:model}
 G = U V^T + X B^T + E 
\end{equation}

Here the G $n \times L$ matrix records n observations of a vector $G_i = (G_{i
1},..., G_{i L})$ of size L (genotype, methylation level). Variation of the
observations variable are explained by K latent factor ($U V^T$) and the
interest co-variate ($X$). For each observation the $X$ $n \times d$ matrix
records for each $i$ the observation of $d$ co-variate (phenotype, environmental
gradient). The matrices $U$ of size $n /times K$ records the scores for $K$
latent factors. The $K$ latent factor loading are stored in the $L \times K$
matrix $V$. The $n \times L$ matrix $E$ is the residual error matrix.

A classic method to estimate $U$ $V$ and $B$ is to write the following
optimization problem 

#+NAME: eqn:optim_no_reg
\begin{equation}
min \frac{1}{2} ||G - U V^T - X B^T||_{F}^2 
\end{equation}

where $||.||_{F}$ is the Frobenius norm. This optimization problem arises when
considering the log-likelihood for a Gaussian residual error matrix $E$. 

We can easily show that the equation [[eqn:optim_no_reg]] do not allow to
estimate unique latent factor product $U V^T$ and a unique effect matrix
$B$. Then we can not uniquely identify the part explain by the $K$ latent factor
and the part explain by the co-variate of interest. We have to add a
regularization term to fix that.

*** demo
We write $B^*$ the unique matrices of optimization problem [[eqn:optim_no_reg]],
and $U^*$, $V^*$ possible matrix for this solution. We remark that $U^*$ and
$V^*$ are not unique because $U^* R$ and $V^*R^{-1}$ is still a valid solution
of [[eqn:optim_no_reg]].

Then $U^{**} = U^* + X $ and $B^{**} = B^* - V^*$ is still a valid solution for
[[eqn:optim_no_reg]].

** Ridge regularized estimation
In order to force uniqueness of the latent factor product matrix $C = U V^T$ and
effect size $B$. We can write the following problem

#+NAME: eqn:optim_ridge_reg
\begin{equation}
min_{rk(C) \leq K} \frac{1}{2} ||G - C - X B^T||_{F}^2 + \lambda ||B||^2_2.
\end{equation}

The optimization problem [[eqn:optim_ridge_reg]] has unique solution $C$ and $B$
which can be write

$$ 
C = P_{\lambda}^{-1} * svd_K(P_{\lambda} G )
$$

and 

$$
... 
$$


** Lasso regularized estimation

to enforce the uniqueness of $C$ and $B$, we propose a lasso regularized
optimization problem

#+NAME: eqn:optim_lasso_reg
$$min_{rk(C) \leq K} \frac{1}{2} ||G - C - X B^T||_{F}^2 + \lambda ||B||_1.$$ 

Contrary to [[eqn:optim_ridge_reg]], we can not write analytic solution of
[[eqn:optim_lasso_reg]]. Moreover the problem is not convex which not enable to
easily guarantee uniqueness of the solution and find an efficient algorithm. To
avoid this problem we can make a convex relaxation of the optimization problem

#+NAME: eqn:optim_convex_lasso_reg
$$min \frac{1}{2} ||G - C - X B^T||_{F}^2 + \lambda ||B||_1 +
\gamma ||C||_*.$$ 

*** An alternated algorithm
In this paragraph we introduce an alternated algorithm which return solution of
[[eqn:optim_convex_lasso_reg]]. This algorithm start with null matrix and
alternate the two step:
- compute $B_{i} as the solution of the lasso linear regression problem
$$
min \frac{1}{2} ||(G - C_{i-1}) - X B^T||_{F}^2 + \lambda ||B||_1 
$$
- compute C as the solution of 
min \frac{1}{2} ||(G - X B_i^T)- C_{i-1} ||_{F}^2 + \gamma ||C||_*
$$
which is give by the singular value shrinkage operator.

** COMMENT Estimation
:LOGBOOK:
- Note taken on [2017-05-12 Ven 16:35] \\
  On va plutot faire uen partie pour l'estimateur lasso et un autre pour la ridge
  et une derniere pour les missing values.
:END:
To estimate model parameter we propose the following optimization problem 

$$min_{rk(C) \leq K} \frac{1}{2} ||G - C - X B^T||_{F}^2 + \lambda r(B)$$ 

where
$C$ is the latent matrix such that $C = U V^T$, $||.||_F$ the Frobenius matrix
,$rk(.)$ the function which returns the rank of a matrix and $r$ a convex
regularization function. In this article we discuss the ridge regularization
function $||.||^2_2$ and the lasso regularization function $||.||$.
*** A ridge regularized estimator
**** Analytic solution
*** A lasso regularized estimator
We write the following convex problem: 

$$min \frac{1}{2} ||G - C - X B^T||_{F}^2 + \lambda ||B|| + \gamma ||C||*)$$ 

where ... It is easy to prove that the problem is convex.

** COMMENT Algorithm
   
In this section we propose algorithms to estimate latent matrices $U$ and $V$
and association parameter $B$. We present a very efficient algorithm for the
ridge regularized loss function based on analytic solutions of the
optimization problem. We also present an efficient alternated algorithm for
data with missing values and a general regularization convex function.


*** Missing values
It is frequent that there are missing values in the data matrix. A solution
is to use an imputation algoritm before running the association study. But
this can lead to bias estimation and spurious association pattern introduced
by the imputation algorithm. We propose an algorithm we avoid using a
preliminary imputation method.

** Hyper-parameters choice
:LOGBOOK:
- Note taken on [2017-05-25 Thu 11:52] \\
  Pour ridge faire ma petite heuristic pour trouver lambda.
  Pour lasso aussi (chemin de reg).
- Note taken on [2017-05-25 Thu 11:49] \\
  Pour une estimation precise des parametre il y a la cross validation. Sinon
  comme la méthode resemble a l'acp auquel on a enlevé la variance expliqué par X
  on peut utiliser les même éthodes que pour l'acp. Quite à surestimer le nombre
  de facteur lattent.
- Note taken on [2017-05-25 Thu 11:46] \\
  Bien preciser que on veut a tou pris eviter les truc du style j'impute a
  l'arrache avant etc...
:END:
*** Cross validation
:LOGBOOK:
- Note taken on [2017-05-26 Fri 14:46] \\
  cf mon cahier
:END:
Cross validation is a classic method to select hyper-parameter. We propose here
method adapted to our algorithms. 
... 

However, cross validation procedure can be long to run in particular on very big
data set. We propose other procedure to assess hyper-parameters that gave good
results our experiments.

*** Choice of K using singular value
Methods presented in this paper are very close to the Principal Component
Analysis (PCA), we can see them as a PCA $G - X^B$. Thus we can use method use
for the PCA to estimate the number of latent factor. These method lead to an
overestimated number of factor in the model [[ref:eq:model][(1)]] because the co-variate
would be considered as latent variable. However, because the goal of our methods
is to estimate latent variation while protection variation explain by co-variate
$X$, algorithms are robust to overestimated $K$.

For real data set, the number of latent variable $K$ by visualizing the singular
of the $G$ matrix. We chose intentionally softly overestimated to be sure to
consider all the latent variation in the data.
*** Heuristic to choice of $\lambda$ ridge
:LOGBOOK:
- Note taken on [2017-05-26 Fri 14:45] \\
  voir mon cahier et il va falloir normaliser lambda ?? a voir !!C'est chiant car
  j'ai deja lancé les experiences !!
:END:
...
We observed that for a centered and normalized $G$ and $X$ $\lambda = ?$
provided good results in our experiments.

*** Heuristic to choice of $\gamma$ lasso
This hyper-parameter impact the rank of the $C$ matrix. To assess the gamma
value we compute singular values of G $(\mu_1, ..., \mu_n)$. Then we set 

$$
\gamma = \frac{(\mu_K + \mu_{K + 1})}{2} 
$$

for $K$ the chosen number of latent factors. In our experiments, we observed
that for such computed $\gamma$ the rank of $C$ returned by lasso algorithm was
$K$.


*** Heuristic to choice of $\lambda$ lasso
This hyper-parameter impact the number of line set to zero in $B$. We know that
only a part of observe variable $G_j$ are correlated with the variable $X$. So
we can interpret the proportion on non zero line in $B$ as the proportion $p$ of
variable which correlate with $X$. To find the lambda which correspond to the
proportion $p$ we propose an heuristic based on a regularization path of lambda
value. We start with smallest value of $\lambda$ such that 
- $C = D_{\gamma}(G)
- B = argmin ....  = 0

Then we construct a sequence of m values of $\lambda$ decreasing from
$\lambda_max to $\lambda_min$ on the log scale. Typical values are \epsilon =
0.001 and K = 100.

** Hypothesis testing
:LOGBOOK:
- Note taken on [2017-05-25 Thu 11:55] \\
  parler de lm : G ~U + X 
  ET
  la recalibration par mad + median
:END:

*** Linear model with latent factor score
:LOGBOOK:
- Note taken on [2017-05-26 Fri 15:35] \\
  faut que je choississe les notations mieux que ca, je m'enmmèle la ...
:END:
After computing latent factors score matrix $U$ with the lasso or ridge
algorithm, we use them as co-variables with $X$ in a linear model. This enable
to compute the pvalue to test the null hypothesis 
$$
B_j = 0
$$

where in $B$ is the ....

*** Hypothesis calibration
Even with latent factors correction we can observed not calibrated p-value. This
can be due to model misspecification, presence of not interested and small
effects or dependency between variables. As we are typically interested by a small
proportion of variables we used empirical correction to have score with a mean
to zero and standard deviation to 1. We used the median and the mad as robust
estimators of the mean and standard deviation.

** Similar methods
*** lm and lm + pca
We comparared results of our method to two well known method the linear model
and the linear model with PCA scores. 
*** cate

*** sva
*** famt
** Simulations and data

*** Generative model simulation
We used equation to generate generative model dataset. The latent factor
scores and loadings $U$ and $V$ were generated using a multivariate gaussian
distribution with a zero mean and a $K$ identity matrix for the covariance
matrix where is the number of latent factor. The error matrix $E$ was
generated using a multivariate gaussian distribution with a zero mean and a
$L$ identity matrix for the covariance matrix where $L$ is the number of
variables. The co-variable $X$ was generated with a normal distribution with
the mean equal to zero and the standard deviation equal to one such that the
Pearson linear correlation between $X$ and $U_1$ the first latent score
matrix equal to $c$.

*** Real data example
In to evaluate our methods on real data we chose realized a genome-wide
association study (GWAS), an genome-wide association study (EWAS) and an
ecological association study (EAS).
**** Association study of DNA methylation with rheumatoid arthritis (EWAS)
In order to evaluate the ability to our method to correct for unobserved
confounding variable we used data from a recent association study of DNA
methylation with rheumatoid arthritis (RA) cite:Liu_2013. For this data set
confounding variables (batch effect, age, gender, smoking status, cell-type
composition). Thus, we can compare our method result with result of method
considering explicitly these variables cite:Rahmani_2016,Zou_2014. We retrieve
the RA data from Gene Expression Omnibus (GEO) database (accession number
GSE42861). Following cite:Zou_2014 we filtered out site if its average probe
$\beta$ value was above 0.8 are below 0.2. Then, the $\beta$ values was centered
and normalized with standard deviation.

**** Association study of genetic variants with Celiac disease (GWAS)

**** Association study of genetic variants with climatic data (EAS)

* Results
* Discussion
* Figures and tables
** Numerical validation

bibliographystyle:unsrt
bibliography:../biblio.bib
