# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE: Méthodes de factorisation matricielle pour la génomique des populations et tests d'association
#+AUTHOR:      Kevin Caye

#+LANGUAGE:  en
#+STARTUP: overview indent inlineimages logdrawer
#+OPTIONS: H:5 author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

# #+LaTeX_CLASS: book
#+LaTeX_CLASS: article
#+LATEX_HEADER: \input{notations.tex}

#+HTML_MATHJAX: align: left indent: 5em tagside: left font: Neo-Euler

#  LocalWords:  methylation polymorphism nucleotide Frobenius invertible SNP
#  LocalWords:  preprocessing dataset RidgeLFMM LassoLFMM

bibliographystyle:unsrt
bibliography:../biblio.bib

* État de l'art 
** Contexte
:LOGBOOK:
- Note taken on [2017-06-05 Mon 10:38] \\
  Ca peut etre cool de replacer le context historique en partant de la niasance
  des stats (fisher etc) et de faire le parallele avec maintenant pour on a
  suffisament de données pour se rendre compte que nos test d'hypothèse sont faux
  :D et la on fait le lien avec les tests d'hypothèe multiple....
:END:
** Analyse factorielle des données biologiques
** Test d'association
** Problématique et plan
* Inférence rapide des coefficients de métissage à l'aide des données géographique
:LOGBOOK:
- Note taken on [2017-06-05 Mon 13:44] \\
  Ce qui serais stylé c'est d'ajouté une cross validation propre pour tess3 :D, et
  de relancer les analyse sur AT, voir pk pas sur les très gros dataset AT :D !!!
  
  On ne toucherais pas à l'autre papier mais on lance sur ce dataset la même
  analyse mais très proprement :D, y compris pour l'étude stat à la fin
  (recalibration propre !)
:END:
* Estimation des facteurs latents pour corriger les test d'association :3Article:
** Introduction
** Methods
*** Model 

We present here the model and the notations which will be used in the article.
Following the common notation in latent factor mixed model (LFMM) we write the following
model
\begin{equation}
\label{eq:model}
\Y = \X \B^T + \U \V^T + \E 
\end{equation}
Here $\Y$ is a $\Yrow\times\Ycol$ observed output matrix. For instance, output can
be single nucleotide polymorphism (SNP) or methylation level. The $\Yrow\times\Xcol$
matrix $\X$ records observed primary variables. Primary variables can be for
example disease state or environmental gradient. The $\Ycol\times\Xcol$ matrix $\B$
records primary effects. Matrices $\U$ and $\V$ are respectively $\n \times \K$
score matrix and $\Ycol\times\Ucol$ loading matrix of a regression with $\Ucol$ latent
factors. The matrix $\E$ is the residual error matrix of size $\Yrow\times\Ycol$.


A classic method to estimate $\U$ $\V$ and $\B$ is to write the following
loss function
\begin{equation}
\label{eq:optim_no_reg}
\LfmmL
\end{equation}
where $\norm{.}_{F}$ is the Frobenius norm. This loss function arises when
considering the log-likelihood if $\E$ rows are independent and Gaussian 
with means $0$ and $\Var(\E_{i,j}) = \sigma$.

However, the function $L$ do not allow to define unique factor matrices product
$$\C = \U \V^T$$ and a unique primary effect matrix $\B$. It means that we can
not identify the output variance explained by the $K$ latent factors and the
output variance explained by $\X$.

We will now expose two regularized variants of function $L$ which lead to
unique primary effect matrix $\B$. 

**** demo
We write $\hat{\B}$ the unique matrices of optimization problem defined by the
loss function $\L$ in [[eqref:eq:optim_no_reg]], and $\hat{\U}$, $\hat{\V}$
possible matrices for this solution. First, we remark that $\hat{\U}$ and
$\hat{\V}$ are not unique because, for any invertible matrix $\matr{R}$ we have
$$L(\hat{\U}, \hat{\V}^{T}, \hat{\B}) = L(\hat{\U} \matr{R}, \matr{R}^{-1}
\V^{T}, \hat{\B}).$$ Then, we remark that for a any $\Xcol \times \Ycol$ matrix
$\matr{C}$
\begin{equation*}
L(\hat{\U} - \X \matr{C}, \hat{\V}^{T}, \hat{\B} + \hat{\V} \matr{C}^T}) = L(\hat{\U},
\hat{\V}^{T}, \hat{\B})
\end{equation*}
Thus, $\hat{\B}$ is not unique.


*** Ridge regularized latent factor mixed model (RidgeLFMM)
In this section we present a ridge regularized latent factor mixed model
(RidgeLFMM). The regularized loss function is written as follow 
\begin{equation}
\label{eq:optim_ridge_reg}
\LfmmLridge
\end{equation}
We can show that minimizing the function eqref:eq:optim_ridge_reg leads to the
following unique solution
\begin{align*}
\hat{\U} \hat{\V} & =  \sqrt{\obP}^{-1} * svd_{\K}(\sqrt{\obP} \Y ) \\
\hat{\B} & = (\X^{T} \X + \lambda \Id_{d})^{-1} \X^{T} (G - \hat{\U} \hat{\V}).
\end{align*}
Where $svd_{\K}(\matr{A})$ is the best approximation of rank $\K$ for the matrix
$\matr{A}$ given by the singular value decomposition. $\Id_{q}$ is the $q \times
q$ identity matrix. The $\Yrow \times \Yrow$ matrix $\obP$ is an oblique
projection matrix defined as $$\LfmmP.$$ The matrix $\sqrt{\obP}$ is define such that 
\begin{equation*}
\obP = \sqrt{\obP}^{2}
\end{equation*}


**** Proof
We want to find 
\begin{align*}
\hat{\U} & \in \RR^{\Urow \times \Ucol} \\
\hat{\V} & \in \RR^{\Vrow \times \Vcol} \\
\hat{\B} & \in \RR^{\Brow \times \Bcol} \\
\end{align*}
which minimize the function $L_{ridge}$ define in eqref:eq:optim_ridge_reg. We
start to compute the Jacobian of $L_{ridge}$ along $\B$, $\U$ and $\V$
\begin{equation}
\begin{cases}
\label{eq:partial_Lridge}
& \frac{\partial L_{ridge}}{\partial \B}(\U, \V, \B) = \X^{T} (\U \V^{T} + \X \B^{T} - \Y) + \lambda \Id_{d} \B^{T} \\
& \frac{\partial L_{ridge}}{\partial \V}(\U, \V, \B) = \U^{T} (\U \V^{T} + \X \B^{T} - \Y) \\
& \frac{\partial L_{ridge}}{\partial \U}(\U, \V, \B) = (\U \V^{T} + \X \B^{T} - \Y) \V

\end{cases}
\end{equation}
The minimum is reach if and even if  eqref:eq:partial_Lridge equal to
zero. We can write
\begin{equation}
\begin{cases}
\label{eq:euler_Lridge}
& \hat{\B}^{T} = (\X^{T} \X + \lambda \Id_{\Bcol})^{-1} \X^{T} (\Y - \U \V) \\
& 0 = \U^{T} (\U \V^{T} + \X \B^{T} - \Y) \\
& 0 = (\U \V^{T} + \X \B^{T} - \Y) \V
\end{cases}
\end{equation}
Then by using the first line of eqref:eq:euler_Lridge the two last ones we have
\begin{equation}
\label{eq:euler_UV_Lridge}
\begin{cases}
&  0 = \hat(\U)^{T} \obP (\hat{\U} \hat{\V}^{T} - \Y) \\
& 0 = \obP (\hat{\U} \hat{\V}^{T} - \Y) \hat{\V}
\end{cases}
\end{equation}
Finally, finding the solutions $\hat{\U}$ and $\hat{\V}$ of
eqref:eq:euler_UV_Lridge is equivalent to find the minimum of 
\begin{equation}
L^{'}_{ridge}(\U, \V) = \frac{1}{2} \norm{ \sqrt{\obP} (\Y - \U \V^{T})}_{F}^{2}
\end{equation}
which is the classic problem of finding $\K$ rank best approximation of the matrix
$$ \sqrt{\obP} \Y.$$
As result we have 
\begin{align*}
\hat{\U} \hat{\V} & =  \sqrt{\obP}^{-1} * svd_{\K}(\sqrt{\obP} \Y ) \\
\hat{\B} & = (\X^{T} \X + \lambda \Id_{d})^{-1} \X^{T} (G - \hat{\U} \hat{\V}).
\end{align*}

*** Lasso regularized latent factor mixed model (LassoLFMM)
In this section we present a lasso regularized latent factor mixed model (LassoLFMM)
The regularized loss function is written as follow
\begin{equation}
\label{eq:optim_lasso_reg}
\LfmmLlasso
\end{equation}
Where $\norm{.}_{*}$ is the nuclear norm. Contrary to $L_{ridge}$ finding minimum
solution of $L_{lasso}$ is not easy. However, if we made the variable change $$ \C = \U \V^{T}
$$ in ref:eq:optim_lasso_reg, the function $L_{lasso}$ become an convex
function of $\C$ and $\B$. Thereby, we can apply alternated algorithm to compute
minimum value of $L_{lasso}$.

**** An alternated algorithm
We start with null matrices
\begin{align*}
C_{t = 0} & = 0 \\
B_{t = 0} & = 0.
\end{align*}
Then we alternate the two steps 
- compute $\B_{t}$ as minimizing the loss function
\begin{equation}
\label{eq:lasso_algo_1}
L_{lasso}^{1}(\B) =  \frac{1}{2} ||(\Y - \C_{t-1}) - \X \B^T||_{F}^2 + \lambda ||\B||_1
\end{equation}
- compute $\C_{t}$ as minimizing the loss function
\begin{equation}
\label{eq:lasso_algo_2}
L_{lasso}^{2}(\C) = \frac{1}{2} ||(\Y - \X \B_t^T)- \C ||_{F}^2 + \gamma ||\C||_{*}.
\end{equation}

The first step is a regression of $$\Y - \C_{t-1}$$ by the primary variable $\X$ with
a lasso regularization on the primary effect matrix $\B$. The second step is a
low rank approximation of the regression residual $$\Y - \X \B_{t}^{T}.$$ The low
rank approximation is given by the singular value shrinkage operator, see
cite:cai10_singul_value_thres_algor_matrix_compl.
*** Choice of hyper-parameters
:LOGBOOK:
- Note taken on [2017-05-25 Thu 11:52] \\
  Pour ridge faire ma petite heuristic pour trouver lambda.
  Pour lasso aussi (chemin de reg).
- Note taken on [2017-05-25 Thu 11:49] \\
  Pour une estimation precise des parametre il y a la cross validation. Sinon
  comme la méthode resemble a l'acp auquel on a enlevé la variance expliqué par X
  on peut utiliser les même éthodes que pour l'acp. Quite à surestimer le nombre
  de facteur lattent.
- Note taken on [2017-05-25 Thu 11:46] \\
  Bien preciser que on veut a tou pris eviter les truc du style j'impute a
  l'arrache avant etc...
:END:

LassoLFMM and RidgeLFMM .....

**** Cross validation
:LOGBOOK:
- Note taken on [2017-05-26 Fri 14:46] \\
  cf mon cahier
:END:
Cross validation is a classic method to select hyper-parameter in factor
analysis cite:Owen_2009,Owen_2009,Bro_2008. We explain here a cross-validation
method adapted to our methods.

[...]

However, cross validation procedure can be long to run in particular on very big
data set. Especially since, LassoLFMM and RidgeLFMM have each 2 hyper-parameters
which can be cross-validated.We propose other procedure to assess
hyper-parameters that gave good results our experiments.

**** Choice of K using singular value
Methods presented in this paper are very close to the Principal Component
Analysis (PCA), we can seen them as a PCA of $G - X^B$. Thus we propose to use
select the number of latent variable $\K$ by visualizing the scree plot. 

We empirically observed that, this method leads to an overestimated number of
factor in the model describe in eqref:eq:model since the co-variate would be
considered as a latent variable. However, because the goal of our methods is to
estimate latent variation while protection variation explain by co-variate $X$,
we observed that our algorithms was robust to overestimated $K$.


...

**** Heuristic to choice of $\lambda$ ridge
:LOGBOOK:
- Note taken on [2017-06-01 jeu. 12:03] \\
  et la on fait le lien avec le model de cate :D
- Note taken on [2017-05-26 Fri 14:45] \\
  voir mon cahier et il va falloir normaliser lambda ?? a voir !!C'est chiant car
  j'ai deja lancé les experiences !!
:END:
In the article of \MethodCate method cite:wang2015confounder, authors propose to
explicitly model the relationship between the factor score matrix $\U$ and the
primary variables matrix $\X$. There assume that there is a linear relationship
between $\U$ and $\X$ such as $$ \U = \X \matr{\alpha}^{T} + matr{W},$$ where
$\W$ is a $\Urow \times \K$ residual error matrix independent of $\X$ and $\E$
and $matr{\alpha}$ $\Xcol \times \Ucol$ characterizes the linear relationship
between $\U$ and $\X$. If $\matr{\alpha}$ is null, there is no problem of
confounding and $\U$, $\V$ and $\X$ can be estimated separately.  


...
We observed that for a centered and normalized $G$ and $X$ $\lambda = ?$
provided good results in our experiments.

**** Heuristic to choice of $\gamma$ lasso
This hyper-parameter impact the rank of the $C$ matrix. To assess the gamma
value we compute singular values of G $(\mu_1, ..., \mu_n)$. Then we set 

$$
\gamma = \frac{(\mu_K + \mu_{K + 1})}{2} 
$$

for $K$ the chosen number of latent factors. In our experiments, we observed
that for such computed $\gamma$ the rank of $C$ returned by lasso algorithm was
$K$.


**** Heuristic to choice of $\lambda$ lasso
This hyper-parameter impact the number of line set to zero in $B$. We know that
only a part of observe variable $G_j$ are correlated with the variable $X$. So
we can interpret the proportion on non zero line in $B$ as the proportion $p$ of
variable which correlate with $X$. To find the lambda which correspond to the
proportion $p$ we propose an heuristic based on a regularization path of lambda
value. We start with smallest value of $\lambda$ such that 
- $C = D_{\gamma}(G)
- B = argmin ....  = 0

Then we construct a sequence of m values of $\lambda$ decreasing from
$\lambda_max to $\lambda_min$ on the log scale. Typical values are \epsilon =
0.001 and K = 100.

*** Hypothesis testing
:LOGBOOK:
- Note taken on [2017-05-25 Thu 11:55] \\
  parler de lm : G ~U + X 
  ET
  la recalibration par mad + median
:END:

**** Linear model with latent factor score
:LOGBOOK:
- Note taken on [2017-05-26 Fri 15:35] \\
  faut que je choississe les notations mieux que ca, je m'enmmèle la ...
:END:
After computing latent factors score matrix $U$ with the lasso or ridge
algorithm, we use them as co-variables with $X$ in a linear model. This enable
to compute the pvalue to test the null hypothesis 
$$
B_j = 0
$$

where in $B$ is the ....

**** Hypothesis calibration
:LOGBOOK:
- Note taken on [2017-06-01 jeu. 14:45] \\
  Voir dans cite:gerard2017unifying la parti sur la calibration !
:END:
Even with latent factors correction we can observed not calibrated p-value. This
can be due to model misspecification, presence of not interested and small
effects or dependency between variables. As we are typically interested by a small
proportion of variables we used empirical correction to have score with a mean
to zero and standard deviation to 1. We used the median and the mad as robust
estimators of the mean and standard deviation.


** Simulation study and dataset
:PROPERTIES:
:header-args: :cache no :eval no-export :results output :exports none
:END:
*** Others methods
<<sec:similar_method>>
**** lm and lm + pca
We comparared results of our method to two well known method the linear model
and the linear model with PCA scores. 
**** cate

**** sva
**** famt
*** Simulations and data

**** Generative model simulation

We used equation to generate generative model dataset. The latent factor
scores and loadings $U$ and $V$ were generated using a multivariate gaussian
distribution with a zero mean and a $K$ identity matrix for the covariance
matrix where is the number of latent factor. The error matrix $E$ was
generated using a multivariate gaussian distribution with a zero mean and a
$L$ identity matrix for the covariance matrix where $L$ is the number of
variables. The co-variable $X$ was generated with a normal distribution with
the mean equal to zero and the standard deviation equal to one such that the
Pearson linear correlation between $X$ and $U_1$ the first latent score
matrix equal to $c$.

**** Real data example
In this section we present the real data we used to compare lasso LFMM, ridge
LFMM with similar methods presented in section [[sec:similar_method]]. To evaluate
the utility of our methods on several situation we select study where correction
for confounding variables is an important step. We realized genome wide
association study (GWAS), an genome-wide association study (EWAS) and an
ecological association study (EAS). Before running algorithm $\G$ and $\X$
matrix was centered and normalized with standard deviation for all the study. We
now describe preprocessing step for each study.

***** Association study of DNA methylation with rheumatoid arthritis (EWAS)
For the EWAS we chose data from a recent association study of DNA methylation with
rheumatoid arthritis (RA) cite:Liu_2013. We retrieve the RA data from Gene
Expression Omnibus (GEO) database (accession number GSE42861). Following
cite:Zou_2014 we filtered out site if its average probe $\beta$ value was above
0.8 are below 0.2. We finally obtain $n = 689$ and $L = 162038$.

#+BEGIN_SRC R :session *ssh krakenator*
  G <- readRDS("~/Projects/Thesis/Data/ThesisDataset/3Article/GSE42861/G.rds")
  dim(G)
#+END_SRC

#+RESULTS:
: [1]    689 162038

For this data set confounding variables (batch effect, age, gender, smoking
status, cell-type composition) are known but we did not use them in methods.
Thus, we can compare methods output with output of method considering explicitly
these variables cite:Rahmani_2016,Zou_2014.

***** Association study of genetic variants with Celiac disease (GWAS)
For the GWAS we chose data from an association study of SNPs with Celiac disease
citep:dubois2010multiple. Before running method we apply classic preprossessing
step with the software Plink cite:Purcell_2007. Firstly, we keep only individual
and SNPs with a proportion of missing value inferior to $5\%$. Then, we filter
out variants with minor allele frequency below $0.05$ and Hardy-Weinberg
equilibrium exact test \pvalue below $1e-10$. After that we filter out
individuals which have identity-by-descent proportion (first by pairs) superior
to $0.08$. Finally, we perform an linkage disequilibrium pruning to obtain SNPs
which are not correlated. The final dataset was of size $n = $ and $L = $.

#+BEGIN_SRC R :session *ssh krakenator*
  G <- readRDS("~/Projects/Thesis/Data/ThesisDataset/3Article/Celiac/G_clumped.rds")
  dim(G)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 15155 94497
#+end_example

We also impute missing value with the sowtware 

***** Association study of genetic variants with climatic data (EAS)
For EAS
** Results
** Discussion
** Figures and tables
*** Numerical validation


* Conclusion 


* COMMENT perspectives
:LOGBOOK:
- Note taken on [2017-05-26 Fri 15:49] \\
  Je pense que je ne vais pas pouvoir développer la crossvalidation et les données
  manquante. 
  
  Par contre je peux montrer que si la cross validation est mal faite
  ca abouti a des mauvais choix de parametre (exemple)
  
  Pareil pour les données manquantes. 
  
  Après dans mes application il n'y a jamais trop de données manquantes, donc peut
  être que c'est pas la peine de se prendre la tête... Surtout que la cross
  validation j'en aurai deja parlé !
:END:


